{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from decimal import Decimal\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from sopa.src.solvers.utils import create_solver\n",
    "from sopa.src.models.utils import fix_seeds, RunningAverageMeter\n",
    "from sopa.src.models.odenet_mnist.layers import MetaNODE\n",
    "from sopa.src.models.odenet_mnist.utils import makedirs, learning_rate_with_decay\n",
    "from sopa.src.models.odenet_mnist.data import get_mnist_loaders, inf_generator\n",
    "from MegaAdversarial.src.attacks import (\n",
    "    Clean,\n",
    "    PGD,\n",
    "    FGSM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')\n",
    "parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])\n",
    "parser.add_argument('--activation', type=str, choices=['tanh', 'softplus', 'softsign', 'relu'], default='relu')\n",
    "parser.add_argument('--in_channels', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--solvers',\n",
    "                    type=lambda s: [tuple(map(lambda iparam: str(iparam[1]) if iparam[0] <= 1 else (\n",
    "                        int(iparam[1]) if iparam[0] == 2 else (\n",
    "                            float(iparam[1]) if iparam[0] == 3 else Decimal(iparam[1]))),\n",
    "                                              enumerate(item.split(',')))) for item in s.strip().split(';')],\n",
    "                    default=None,\n",
    "                    help='Each solver is represented with (method,parameterization,n_steps,step_size,u0,v0) \\n' +\n",
    "                         'If the solver has only one parameter u0, set v0 to -1; \\n' +\n",
    "                         'n_steps and step_size are exclusive parameters, only one of them can be != -1, \\n'\n",
    "                         'If n_steps = step_size = -1, automatic time grid_constructor is used \\n;'\n",
    "                         'For example, --solvers rk4,uv,2,-1,0.3,0.6;rk3,uv,-1,0.1,0.4,0.6;rk2,u,4,-1,0.3,-1')\n",
    "\n",
    "parser.add_argument('--solver_mode', type=str, choices=['switch', 'ensemble', 'standalone'], default='standalone')\n",
    "parser.add_argument('--val_solver_modes',\n",
    "                    type=lambda s: s.strip().split(','),\n",
    "                    default=['standalone'],\n",
    "                    help='Solver modes to use for validation step')\n",
    "\n",
    "parser.add_argument('--switch_probs', type=lambda s: [float(item) for item in s.split(',')], default=None,\n",
    "                    help=\"--switch_probs 0.8,0.1,0.1\")\n",
    "parser.add_argument('--ensemble_weights', type=lambda s: [float(item) for item in s.split(',')], default=None,\n",
    "                    help=\"ensemble_weights 0.6,0.2,0.2\")\n",
    "parser.add_argument('--ensemble_prob', type=float, default=1.)\n",
    "\n",
    "parser.add_argument('--noise_type', type=str, choices=['cauchy', 'normal'], default=None)\n",
    "parser.add_argument('--noise_sigma', type=float, default=0.001)\n",
    "parser.add_argument('--noise_prob', type=float, default=0.)\n",
    "parser.add_argument('--minimize_rk2_error', type=eval, default=False, choices=[True, False])\n",
    "\n",
    "parser.add_argument('--nepochs_nn', type=int, default=50)\n",
    "parser.add_argument('--nepochs_solver', type=int, default=0)\n",
    "parser.add_argument('--nstages', type=int, default=1)\n",
    "\n",
    "parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--test_batch_size', type=int, default=1000)\n",
    "parser.add_argument('--base_lr', type=float, default=1e-5, help='base_lr for CyclicLR scheduler')\n",
    "parser.add_argument('--max_lr', type=float, default=1e-3, help='max_lr for CyclicLR scheduler')\n",
    "parser.add_argument('--step_size_up', type=int, default=2000, help='step_size_up for CyclicLR scheduler')\n",
    "parser.add_argument('--cyclic_lr_mode', type=str, default='triangular2', help='mode for CyclicLR scheduler')\n",
    "parser.add_argument('--lr_uv', type=float, default=1e-3)\n",
    "parser.add_argument('--torch_dtype', type=str, default='float32')\n",
    "parser.add_argument('--wandb_name', type=str, default='find_best_solver')\n",
    "\n",
    "parser.add_argument('--data_root', type=str, default='./')\n",
    "parser.add_argument('--save_dir', type=str, default='./')\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=502)\n",
    "# Noise and adversarial attacks parameters:\n",
    "parser.add_argument('--data_noise_std', type=float, default=0.,\n",
    "                    help='Applies Norm(0, std) gaussian noise to each training batch')\n",
    "parser.add_argument('--eps_adv_training', type=float, default=0.3,\n",
    "                    help='Epsilon for adversarial training')\n",
    "parser.add_argument(\n",
    "    \"--adv_training_mode\",\n",
    "    default=\"clean\",\n",
    "    choices=[\"clean\", \"fgsm\", \"at\"],  # , \"at_ls\", \"av\", \"fs\", \"nce\", \"nce_moco\", \"moco\", \"av_extra\", \"meta\"],\n",
    "    help='''Adverarial training method/mode, by default there is no adversarial training (clean).\n",
    "        For further details see MegaAdversarial/train in this repository.\n",
    "        '''\n",
    ")\n",
    "parser.add_argument('--use_wandb', type=eval, default=True, choices=[True, False])\n",
    "parser.add_argument('--ss_loss', type=eval, default=False, choices=[True, False])\n",
    "parser.add_argument('--ss_loss_reg', type=float, default=0.1)\n",
    "parser.add_argument('--timestamp', type=int, default=int(1e6 * time.time()))\n",
    "\n",
    "parser.add_argument('--eps_adv_testing', type=float, default=0.3,\n",
    "                    help='Epsilon for adversarial testing')\n",
    "parser.add_argument('--adv_testing_mode',\n",
    "                    default=\"clean\",\n",
    "                    choices=[\"clean\", \"fgsm\", \"at\"],\n",
    "                    help='''Adversarsarial testing mode''')\n",
    "\n",
    "args = parser.parse_args(['--solvers', 'rk4,u3,4,-1,0.3,-1', '--seed', '902', '--adv_testing_mode', 'at', \n",
    "                          '--max_lr', '0.001', '--base_lr', '1e-05'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtalgat\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.7<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">breezy-violet-42</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sopa_node/find_best_solver\" target=\"_blank\">https://wandb.ai/sopa_node/find_best_solver</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sopa_node/find_best_solver/runs/16j2vx1d\" target=\"_blank\">https://wandb.ai/sopa_node/find_best_solver/runs/16j2vx1d</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20210216_151114-16j2vx1d</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "makedirs(args.save_dir)\n",
    "if args.use_wandb:\n",
    "    wandb.init(project=args.wandb_name, anonymous=\"allow\", entity=\"sopa_node\")\n",
    "    wandb.config.update(args)\n",
    "    wandb.config.update({'u': float(args.solvers[0][-2])})\n",
    "    makedirs(wandb.config.save_dir)\n",
    "    makedirs(os.path.join(wandb.config.save_dir,  wandb.run.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a checkpoint\n",
    "checkpoint_name = './checkpoints/checkpoint_15444.pth'\n",
    "device = f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu'\n",
    "model = torch.load(checkpoint_name, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaNODE(\n",
       "  (downsampling_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (3): Flatten()\n",
       "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): MetaODEBlock(\n",
       "      (rhs_func): ODEfunc(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv1): ConcatConv2d(\n",
       "          (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv2): ConcatConv2d(\n",
       "          (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (norm3): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_eval_loader = get_mnist_loaders(args.data_aug,\n",
    "                                                                     args.batch_size,\n",
    "                                                                     args.test_batch_size,\n",
    "                                                                     data_root=args.data_root)\n",
    "data_gen = inf_generator(train_loader)\n",
    "batches_per_epoch = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.torch_dtype == 'float64':\n",
    "    dtype = torch.float64\n",
    "elif args.torch_dtype == 'float32':\n",
    "    dtype = torch.float32\n",
    "    \n",
    "solvers = [create_solver(*solver_params, dtype=dtype, device=device) for solver_params in args.solvers]\n",
    "for solver in solvers:\n",
    "    solver.freeze_params()\n",
    "    \n",
    "solver_options = Namespace(**{key: vars(args)[key] for key in ['solver_mode', 'switch_probs',\n",
    "                                                                         'ensemble_prob', 'ensemble_weights']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x, K):\n",
    "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
    "\n",
    "def accuracy(model, dataset_loader, device, solvers=None, solver_options=None):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    for x, y in dataset_loader:\n",
    "        x = x.to(device)\n",
    "        y = one_hot(np.array(y.numpy()), 10)\n",
    "        target_class = np.argmax(y, axis=1)\n",
    "        with torch.no_grad():\n",
    "            if solver is not None:\n",
    "                out = model(x, solvers, solver_options).cpu().detach().numpy()\n",
    "            else:\n",
    "                out = model(x).cpu().detach().numpy()\n",
    "        predicted_class = np.argmax(out, axis=1)\n",
    "        total_correct += np.sum(predicted_class == target_class)\n",
    "    return total_correct / len(dataset_loader.dataset)\n",
    "\n",
    "\n",
    "def adversarial_accuracy(model, dataset_loader, device, solvers=None, solver_options=None, args=None):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    if args.adv_testing_mode == \"clean\":\n",
    "        test_attack = Clean(model)\n",
    "    elif args.adv_testing_mode == \"fgsm\":\n",
    "        test_attack = FGSM(model, mean=[0.], std=[1.], **CONFIG_FGSM_TEST)\n",
    "    elif args.adv_testing_mode == \"at\":\n",
    "        test_attack = PGD(model, mean=[0.], std=[1.], **CONFIG_PGD_TEST)\n",
    "    else:\n",
    "        raise ValueError(\"Attack type not understood.\")\n",
    "    for x, y in dataset_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x, y = test_attack(x, y, {\"solvers\": solvers, \"solver_options\": solver_options})\n",
    "        y = one_hot(np.array(y.cpu().numpy()), 10)\n",
    "        target_class = np.argmax(y, axis=1)\n",
    "        with torch.no_grad():\n",
    "            if solver is not None:\n",
    "                out = model(x, solvers, solver_options).cpu().detach().numpy()\n",
    "            else:\n",
    "                out = model(x).cpu().detach().numpy()\n",
    "        predicted_class = np.argmax(out, axis=1)\n",
    "        total_correct += np.sum(predicted_class == target_class)\n",
    "    return total_correct / len(dataset_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = accuracy(model, test_loader, device=device,\n",
    "                         solvers=solvers, solver_options=solver_options)\n",
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PGD_TEST = {\"eps\": 0.3, \"lr\": 2.0 / 255, \"n_iter\": 7}\n",
    "adv_accuracy_test = adversarial_accuracy(model, test_loader, device,\n",
    "                                         solvers=solvers, solver_options=solver_options, args=args\n",
    "                                        )\n",
    "adv_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
